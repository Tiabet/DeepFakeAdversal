{
 "cells": [
  {
   "cell_type": "code",
   "id": "e3e8abf7-e75a-4ab5-a261-36964db0f2fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:28:07.208204Z",
     "start_time": "2024-11-25T12:28:07.172757Z"
    }
   },
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the MTCNN model\n",
    "mtcnn = MTCNN(keep_all=True, post_process=False)\n",
    "\n",
    "# resnet = InceptionResnetV1(pretrained='casia-webface').eval()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "mtcnn.eval()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MTCNN(\n",
       "  (pnet): PNet(\n",
       "    (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=10)\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=16)\n",
       "    (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=32)\n",
       "    (conv4_1): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (softmax4_1): Softmax(dim=1)\n",
       "    (conv4_2): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (rnet): RNet(\n",
       "    (conv1): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=28)\n",
       "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(28, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=48)\n",
       "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv3): Conv2d(48, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=64)\n",
       "    (dense4): Linear(in_features=576, out_features=128, bias=True)\n",
       "    (prelu4): PReLU(num_parameters=128)\n",
       "    (dense5_1): Linear(in_features=128, out_features=2, bias=True)\n",
       "    (softmax5_1): Softmax(dim=1)\n",
       "    (dense5_2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       "  (onet): ONet(\n",
       "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu1): PReLU(num_parameters=32)\n",
       "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu2): PReLU(num_parameters=64)\n",
       "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (prelu3): PReLU(num_parameters=64)\n",
       "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (conv4): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (prelu4): PReLU(num_parameters=128)\n",
       "    (dense5): Linear(in_features=1152, out_features=256, bias=True)\n",
       "    (prelu5): PReLU(num_parameters=256)\n",
       "    (dense6_1): Linear(in_features=256, out_features=2, bias=True)\n",
       "    (softmax6_1): Softmax(dim=1)\n",
       "    (dense6_2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    (dense6_3): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:28:37.155150Z",
     "start_time": "2024-11-25T12:28:37.088040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image = Image.open(\"unnamed.jpg\")\n",
    "# Detect faces in the image\n",
    "boxes, probs = mtcnn.detect(image)\n",
    "\n",
    "# # If faces are detected, extract embeddings\n",
    "# if boxes is not None:\n",
    "#     aligned = mtcnn(image)\n",
    "#     embeddings = resnet(aligned).detach()"
   ],
   "id": "af19b78b1c5d8134",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T12:28:42.415658Z",
     "start_time": "2024-11-25T12:28:38.603914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "img = Image.open(\"unnamed.jpg\") \n",
    "# If faces are detected, 'boxes' will contain the bounding box coordinates\n",
    "if boxes is not None:\n",
    "    for box in boxes:\n",
    "        # Draw bounding boxes on the image\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        draw.rectangle(box.tolist(), outline='red', width=3)\n",
    "\n",
    "# Display or save the image with detected faces\n",
    "img.show()  "
   ],
   "id": "49b80b4c32b32061",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T11:57:57.118904Z",
     "start_time": "2024-11-25T11:57:57.101001Z"
    }
   },
   "cell_type": "code",
   "source": "img.size",
   "id": "4c2e898f28d8cc86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(379, 512)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T11:22:58.703814Z",
     "start_time": "2024-11-25T11:22:58.699764Z"
    }
   },
   "cell_type": "code",
   "source": "probs",
   "id": "f579252857b77feb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.999933123588562], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T09:29:36.220465Z",
     "start_time": "2024-11-25T09:29:36.195894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "\n",
    "original_image = cv2.imread(\"unnamed.jpg\")\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Convert image to tensor\n",
    "image_tensor = torch.from_numpy(original_image).permute(2, 0, 1).float() / 255.0\n",
    "image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "image_tensor.requires_grad = True\n"
   ],
   "id": "82b9c910c0d28b30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Initial image_tensor grad_fn: None\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T09:30:25.514493Z",
     "start_time": "2024-11-25T09:30:25.494119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Forward pass through MTCNN\n",
    "outputs = mtcnn.detect(image, landmarks=False)  # Disable landmarks if not needed\n",
    "\n",
    "if outputs[0] is None or len(outputs[0]) == 0:\n",
    "    print(\"No faces detected!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Define a dummy loss (e.g., targeting a specific bounding box value)\n",
    "target_bbox = torch.tensor([[30, 30, 130, 130]])  # Example target bounding box\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Compute the loss\n",
    "predicted_bbox = torch.tensor(outputs[0])  # Assuming outputs[0] contains bounding boxes\n",
    "loss = loss_fn(predicted_bbox, target_bbox)\n",
    "\n",
    "# Backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Perform FGSM attack\n",
    "epsilon = 0.01  # Define perturbation magnitude\n",
    "data_grad = image.grad.data\n",
    "perturbed_image = fgsm_attack(image, epsilon, data_grad)"
   ],
   "id": "1216c95b6f6a993",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_8764\\1422097978.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Forward pass through MTCNN\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmtcnn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdetect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlandmarks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# Disable landmarks if not needed\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"No faces detected!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py\u001B[0m in \u001B[0;36mdetect\u001B[1;34m(self, img, landmarks)\u001B[0m\n\u001B[0;32m    311\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    312\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 313\u001B[1;33m             batch_boxes, batch_points = detect_face(\n\u001B[0m\u001B[0;32m    314\u001B[0m                 \u001B[0mimg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmin_face_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    315\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrnet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0monet\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\Anaconda\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py\u001B[0m in \u001B[0;36mdetect_face\u001B[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001B[0m\n\u001B[0;32m     81\u001B[0m         \u001B[0moffset\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mboxes_scale\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 83\u001B[1;33m     \u001B[0mboxes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mboxes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     84\u001B[0m     \u001B[0mimage_inds\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_inds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     85\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21330e174e9875f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
